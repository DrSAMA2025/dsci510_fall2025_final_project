{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# MASLD Awareness Tracker - Results Notebook\n",
    "\"This project tracks and analyzes public and scientific awareness of Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD) in relation to FDA drug approvals of Resmetirom and GLP-1 agonists, examining impacts across search trends, scientific literature, social media, stock markets, and news media.\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up matplotlib for notebook display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from config import DATA_DIR, RESULTS_DIR, STUDY_START_DATE, STUDY_END_DATE, FDA_EVENT_DATES\n",
    "\n",
    "# NOTE: All required packages (including gdown) are installed via requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2deb75ed6264dce",
   "metadata": {},
   "source": [
    "# Create directories\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Study Period: {STUDY_START_DATE} to {STUDY_END_DATE}\")\n",
    "print(f\"FDA Events: {FDA_EVENT_DATES}\")\n",
    "print(f\"Data Sources: Google Trends, Stock Data, Reddit, PubMed, Media Cloud\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd4ff2e49d54dc23",
   "metadata": {},
   "source": [
    "# --- Core Analysis Pipeline ---\n",
    "print(\"=== CORE ANALYSIS PIPELINE ===\")\n",
    "from load import get_google_trends_data, get_stock_data, get_reddit_data, get_pubmed_data, get_media_cloud_data\n",
    "from process import process_google_trends, process_stock_data, process_reddit_data, process_pubmed_data\n",
    "from analyze import (analyze_google_trends, analyze_stock_and_events, analyze_reddit_sentiment,\n",
    "                    analyze_pubmed_publication_rate, advanced_pubmed_analysis,\n",
    "                    advanced_google_trends_analysis, validate_statistical_assumptions, advanced_reddit_sentiment_analysis,\n",
    "                    analyze_reddit_topics, analyze_temporal_patterns, correlate_reddit_trends,\n",
    "                    analyze_subreddit_networks, calculate_statistical_power, create_reddit_summary_table, advanced_stock_analysis, advanced_stock_volatility_analysis, cross_platform_correlation_analysis, advanced_media_cloud_event_analysis, advanced_media_cloud_concentration_analysis, advanced_media_cloud_topic_propagation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44f31a637f2a4122",
   "metadata": {},
   "source": [
    "# Install required package for Google Drive fallback\n",
    "!pip install gdown"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "379da1387a7561e4",
   "metadata": {},
   "source": [
    "# Check if gdown is available for Google Drive fallback\n",
    "try:\n",
    "    import gdown\n",
    "    gdown_available = True\n",
    "    print(\"gdown package available for Google Drive fallback\")\n",
    "except ImportError:\n",
    "    gdown_available = False\n",
    "    print(\"gdown package not available - install with: pip install gdown\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c49fa45619c0744f",
   "metadata": {},
   "source": [
    "# Google Trends Analysis (with API first, fallback to G-Drive)\n",
    "print(\"=== GOOGLE TRENDS ANALYSIS ===\")\n",
    "\n",
    "# Method 1: API Approach\n",
    "print(\"Attempting API data retrieval...\")\n",
    "try:\n",
    "    trends_data = get_google_trends_data()\n",
    "    if trends_data is not None:\n",
    "        processed_trends = process_google_trends(trends_data)\n",
    "\n",
    "        # === DEBUG FOR API PATH ===\n",
    "        print(\"=== API PATH DEBUG ===\")\n",
    "        print(f\"processed_trends type: {type(processed_trends)}\")\n",
    "        print(f\"processed_trends shape: {processed_trends.shape}\")\n",
    "        print(f\"Index: {processed_trends.index[:3]} ... {processed_trends.index[-3:]}\")\n",
    "        print(f\"Columns: {processed_trends.columns.tolist()}\")\n",
    "        print(f\"Data types:\\n{processed_trends.dtypes}\")\n",
    "        print(\"\\nSample values:\")\n",
    "        for col in processed_trends.columns:\n",
    "            print(f\"  {col}: min={processed_trends[col].min()}, max={processed_trends[col].max()}, mean={processed_trends[col].mean():.3f}\")\n",
    "        print(\"=\"*50)\n",
    "        # === END DEBUG ===\n",
    "\n",
    "        analyze_google_trends(processed_trends, notebook_plot=True)\n",
    "        print(\"Google Trends analysis completed via API\")\n",
    "    else:\n",
    "        raise Exception(\"API returned no data\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"API approach failed: {e}\")\n",
    "\n",
    "    # Method 2: Google Drive Fallback\n",
    "    print(\"Falling back to Google Drive data...\")\n",
    "    try:\n",
    "        from load import load_google_trends_from_drive\n",
    "        drive_data = load_google_trends_from_drive()\n",
    "\n",
    "        if drive_data is not None:\n",
    "            processed_trends = process_google_trends(drive_data)\n",
    "\n",
    "            # Add Data Preview:\n",
    "            print(f\"Google Trends data shape: {processed_trends.shape}\")\n",
    "            print(\"First 3 rows of Google Trends data:\")\n",
    "            print(processed_trends.head(3))\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            # === DEBUG FOR DRIVE PATH ===\n",
    "            print(\"=== DRIVE PATH DEBUG ===\")\n",
    "            print(f\"processed_trends type: {type(processed_trends)}\")\n",
    "            print(f\"processed_trends shape: {processed_trends.shape}\")\n",
    "            print(f\"Index: {processed_trends.index[:3]} ... {processed_trends.index[-3:]}\")\n",
    "            print(f\"Columns: {processed_trends.columns.tolist()}\")\n",
    "            print(f\"Data types:\\n{processed_trends.dtypes}\")\n",
    "            print(\"\\nSample values:\")\n",
    "            for col in processed_trends.columns:\n",
    "                print(f\"  {col}: min={processed_trends[col].min()}, max={processed_trends[col].max()}, mean={processed_trends[col].mean():.3f}\")\n",
    "            print(\"=\"*50)\n",
    "            # === END DEBUG ===\n",
    "\n",
    "            # Import and configure matplotlib explicitly\n",
    "            import matplotlib.pyplot as plt\n",
    "            get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "            # Run analysis and force display\n",
    "            analyze_google_trends(processed_trends, notebook_plot=True)\n",
    "\n",
    "            save_path = RESULTS_DIR / GOOGLE_TRENDS_ANALYSIS_SUBDIR / \"google_trends_basic_analysis.png\"\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Basic plot saved to: {save_path}\")\n",
    "\n",
    "            # Ensure plot displays\n",
    "            plt.show()\n",
    "            print(\"Google Trends analysis completed using Google Drive data\")\n",
    "        else:\n",
    "            print(\"Google Drive approach failed\")\n",
    "\n",
    "    except Exception as drive_error:\n",
    "        print(f\"Google Drive approach failed: {drive_error}\")\n",
    "\n",
    "print(\"Google Trends EDA completed. Run advanced analysis in next cell.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if processed_trends exists and has good data\n",
    "print(\"Data Status Check:\")\n",
    "print(f\"processed_trends exists: {'processed_trends' in locals()}\")\n",
    "if 'processed_trends' in locals():\n",
    "    print(f\"Data shape: {processed_trends.shape}\")\n",
    "    print(f\"MASLD mean: {processed_trends['MASLD'].mean()}\")\n",
    "    print(f\"NAFLD mean: {processed_trends['NAFLD'].mean()}\")\n",
    "else:\n",
    "    print(\"ERROR: Run Google Trends EDA cell first!\")"
   ],
   "id": "a2eb57f1871a0624",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ADVANCED GOOGLE TRENDS ANALYSIS\n",
    "print(\"Running Advanced Statistical Analysis with Interrupted Time Series...\")\n",
    "\n",
    "from analyze import advanced_google_trends_analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from config import RESULTS_DIR, GOOGLE_TRENDS_ANALYSIS_SUBDIR\n",
    "\n",
    "# Clear any figures and display fresh\n",
    "plt.close('all')\n",
    "\n",
    "# Run analysis ONCE with display enabled\n",
    "print(\"Running comprehensive analysis (including gold-standard ITS)...\")\n",
    "advanced_results = advanced_google_trends_analysis(processed_trends, notebook_plot=True)\n",
    "\n",
    "# Show statistical results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Original t-test results\n",
    "print(\"\\nTRADITIONAL T-TEST RESULTS:\")\n",
    "for event_name, event_data in [('Resmetirom Approval', advanced_results['resmetirom_impact']),\n",
    "                               ('GLP-1 Approval', advanced_results['glp1_impact'])]:\n",
    "    print(f\"\\n{event_name}:\")\n",
    "    for term, stats in event_data.items():\n",
    "        sig = \"SIGNIFICANT\" if stats['p_value'] < 0.05 else \"not significant\"\n",
    "        print(f\"  {term}: {stats['change_absolute']:+.3f} points (p={stats['p_value']:.3f}) [{sig}]\")\n",
    "\n",
    "# ITS Results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GOLD-STANDARD ITS RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'its_analysis' in advanced_results and advanced_results['its_analysis']:\n",
    "    for term, its_result in advanced_results['its_analysis'].items():\n",
    "        print(f\"\\n{term}:\")\n",
    "        if its_result.get('level_resmetirom_p', 1) < 0.05:\n",
    "            print(f\"  Resmetirom Level Change: {its_result['level_change_resmetirom']:.3f} (p={its_result['level_resmetirom_p']:.4f}) *\")\n",
    "        else:\n",
    "            print(f\"  Resmetirom Level Change: {its_result['level_change_resmetirom']:.3f} (p={its_result['level_resmetirom_p']:.4f})\")\n",
    "\n",
    "        if its_result.get('level_glp1_p', 1) < 0.05:\n",
    "            print(f\"  GLP-1 Level Change: {its_result['level_change_glp1']:.3f} (p={its_result['level_glp1_p']:.4f}) *\")\n",
    "        else:\n",
    "            print(f\"  GLP-1 Level Change: {its_result['level_change_glp1']:.3f} (p={its_result['level_glp1_p']:.4f})\")\n",
    "else:\n",
    "    print(\"ITS analysis not available - check function implementation\")"
   ],
   "id": "7930ba3675697cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# GOOGLE TRENDS STATISTICAL ASSUMPTION VALIDATION\n",
    "print(\"=== STATISTICAL ASSUMPTION VALIDATION ===\")\n",
    "\n",
    "if 'processed_trends' in locals():\n",
    "    print(\"\\nValidating statistical assumptions for Resmetirom approval...\")\n",
    "    resmetirom_validation = validate_statistical_assumptions(\n",
    "        processed_trends,\n",
    "        FDA_EVENT_DATES['Resmetirom Approval'],\n",
    "        ['MASLD', 'NAFLD', 'Rezdiffra']\n",
    "    )\n",
    "\n",
    "    print(\"\\nValidating statistical assumptions for GLP-1 approval...\")\n",
    "    glp1_validation = validate_statistical_assumptions(\n",
    "        processed_trends,\n",
    "        FDA_EVENT_DATES['GLP-1 Agonists Approval'],\n",
    "        ['MASLD', 'NAFLD', 'Wegovy', 'Ozempic']\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: processed_trends not found\")"
   ],
   "id": "c1392f5c5c19a83a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# STEP 1: Seasonal Decomposition\n",
    "print(\"=== SEASONAL DECOMPOSITION ANALYSIS ===\")\n",
    "\n",
    "from analyze import analyze_trends_seasonal_decomposition\n",
    "\n",
    "if 'processed_trends' in locals():\n",
    "    decomposition_results = analyze_trends_seasonal_decomposition(processed_trends, notebook_plot=True)\n",
    "    print(\"Seasonal decomposition completed - reveals weekly patterns in search behavior\")\n",
    "else:\n",
    "    print(\"Error: processed_trends not found\")"
   ],
   "id": "f285ea9c624137eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# STEP 2: Non-Parametric Validation\n",
    "print(\"=== NON-PARAMETRIC VALIDATION ===\")\n",
    "\n",
    "from analyze import non_parametric_validation\n",
    "\n",
    "if 'processed_trends' in locals():\n",
    "    print(\"\\nResmetirom Approval (Non-parametric):\")\n",
    "    resmetirom_nonparametric = non_parametric_validation(\n",
    "        processed_trends,\n",
    "        FDA_EVENT_DATES['Resmetirom Approval'],\n",
    "        ['MASLD', 'NAFLD', 'Rezdiffra']\n",
    "    )\n",
    "\n",
    "    print(\"\\nGLP-1 Approval (Non-parametric):\")\n",
    "    glp1_nonparametric = non_parametric_validation(\n",
    "        processed_trends,\n",
    "        FDA_EVENT_DATES['GLP-1 Agonists Approval'],\n",
    "        ['MASLD', 'NAFLD', 'Wegovy', 'Ozempic']\n",
    "    )\n",
    "\n",
    "    # Compare with parametric results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"COMPARISON: Parametric vs Non-Parametric Results\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Consistent results across methods validate our findings\")\n",
    "else:\n",
    "    print(\"Error: processed_trends not found\")"
   ],
   "id": "111a51bc660c9b89",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc48c4a98286631a",
   "metadata": {},
   "source": [
    "# REDDIT ANALYSIS - FULL API DEMONSTRATION\n",
    "# This block demonstrates complete API data collection and processing\n",
    "# Note: This may take 5-10 minutes to complete due to API rate limits\n",
    "# For faster testing, use the \"QUICK ANALYSIS\" block below\n",
    "\n",
    "print(\"=== REDDIT ANALYSIS - FULL API DEMONSTRATION ===\")\n",
    "print(\"This demonstrates complete Reddit API data collection and processing\")\n",
    "print(\"Note: This may take several minutes due to API rate limits\")\n",
    "\n",
    "# Method 1: Complete API Approach\n",
    "print(\"Starting API data collection...\")\n",
    "try:\n",
    "    reddit_data = get_reddit_data()\n",
    "    if reddit_data is not None:\n",
    "        print(\"API data collection completed successfully\")\n",
    "        print(f\"Collected {len(reddit_data)} records from Reddit API\")\n",
    "\n",
    "        # Process the API data\n",
    "        print(\"Processing API data for sentiment analysis...\")\n",
    "        processed_reddit = process_reddit_data(reddit_data)\n",
    "\n",
    "        # Generate analysis and visualizations\n",
    "        print(\"Generating sentiment analysis and visualizations...\")\n",
    "        analyze_reddit_sentiment(processed_reddit, notebook_plot=True)\n",
    "\n",
    "        print(\"Reddit sentiment analysis completed using full API pipeline\")\n",
    "        print(\"This demonstrates successful API integration and data processing\")\n",
    "\n",
    "    else:\n",
    "        print(\"API returned no data - this demonstrates API limitations\")\n",
    "        raise Exception(\"API returned no data\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"API approach encountered issues: {e}\")\n",
    "    print(\"This demonstrates real-world API challenges that require fallback solutions\")\n",
    "    print(\"For reliable analysis, use the QUICK ANALYSIS block with pre-collected data\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb7ebf14b19db029",
   "metadata": {},
   "source": [
    "# REDDIT ANALYSIS - DEVELOPMENT VERSION WITH PRE-COLLECTED DATA\n",
    "# This block uses pre-collected Google Drive data for faster development and testing\n",
    "# Rationale: Reddit API has rate limits and structural issues that slow down development\n",
    "# Use this for reliable, fast analysis during project development\n",
    "\n",
    "print(\"=== REDDIT ANALYSIS - DEVELOPMENT VERSION ===\")\n",
    "print(\"Using pre-collected data for fast, reliable analysis during development\")\n",
    "print(\"Google Drive fallback ensures consistent results despite API limitations\")\n",
    "\n",
    "try:\n",
    "    from load import load_reddit_data_from_drive\n",
    "    drive_data = load_reddit_data_from_drive()\n",
    "\n",
    "    if drive_data is not None:\n",
    "        print(f\"Pre-collected data loaded: {drive_data.shape} records available\")\n",
    "        print(f\"Data columns: {drive_data.columns.tolist()}\")\n",
    "\n",
    "        # Check if data is already in correct format\n",
    "        if 'post_text' in drive_data.columns or 'text_to_analyze' in drive_data.columns:\n",
    "            print(\"Data is already in processed format - proceeding directly to analysis\")\n",
    "            fixed_df = drive_data\n",
    "        else:\n",
    "            print(\"Data requires processing from nested format...\")\n",
    "            fixed_data = []\n",
    "            for idx, row in drive_data.iterrows():\n",
    "                for col in range(len(row)):\n",
    "                    cell_data = row.iloc[col]  # Use .iloc to avoid the warning\n",
    "                    if cell_data is not None and isinstance(cell_data, str) and cell_data.startswith('{'):\n",
    "                        try:\n",
    "                            import ast\n",
    "                            parsed_data = ast.literal_eval(cell_data)\n",
    "                            if isinstance(parsed_data, dict):\n",
    "                                fixed_data.append(parsed_data)\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "            if fixed_data:\n",
    "                import pandas as pd\n",
    "                fixed_df = pd.DataFrame(fixed_data)\n",
    "            else:\n",
    "                print(\"Using original data structure since nested extraction failed\")\n",
    "                fixed_df = drive_data\n",
    "\n",
    "        print(f\"Data ready for analysis: {fixed_df.shape}\")\n",
    "        print(f\"Final columns: {fixed_df.columns.tolist()}\")\n",
    "\n",
    "        # Process and analyze the data\n",
    "        from process import process_reddit_data\n",
    "        from analyze import analyze_reddit_sentiment\n",
    "\n",
    "        processed_reddit = process_reddit_data(fixed_df)\n",
    "\n",
    "        # Add Data Preview:\n",
    "        print(f\"Reddit data shape after processing: {processed_reddit.shape}\")\n",
    "        print(\"First 3 rows of Reddit sentiment data:\")\n",
    "        print(processed_reddit[['subreddit', 'timestamp', 'sentiment_score']].head(3))\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        analyze_reddit_sentiment(processed_reddit, notebook_plot=True)\n",
    "        print(\"Reddit sentiment analysis completed successfully using pre-collected data\")\n",
    "        print(\"This approach ensures consistent results for development and demonstration\")\n",
    "    else:\n",
    "        print(\"Google Drive data unavailable - check file permissions\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Development analysis failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ADVANCED REDDIT SENTIMENT ANALYSIS\n",
    "print(\"Running Advanced Reddit Sentiment Analysis...\")\n",
    "\n",
    "from analyze import advanced_reddit_sentiment_analysis\n",
    "\n",
    "# Make sure processed_reddit exists from previous cell\n",
    "if 'processed_reddit' in locals():\n",
    "    advanced_reddit_results = advanced_reddit_sentiment_analysis(processed_reddit, notebook_plot=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENHANCED REDDIT SENTIMENT RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Display key findings with statistical context\n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Total posts/comments analyzed: {advanced_reddit_results['overall_stats']['total_posts']}\")\n",
    "    print(f\"  Average sentiment: {advanced_reddit_results['overall_stats']['mean_sentiment']:.3f}\")\n",
    "    print(f\"  Date range: {advanced_reddit_results['overall_stats']['date_range']}\")\n",
    "\n",
    "    print(f\"\\nFDA Event Impacts (Statistical Validation Complete):\")\n",
    "    for event_name, impact in advanced_reddit_results['event_impacts'].items():\n",
    "        sig = \"SIGNIFICANT\" if impact['p_value'] < 0.05 else \"not significant\"\n",
    "        print(f\"  {event_name}:\")\n",
    "        print(f\"    Sentiment change: {impact['change_absolute']:+.3f}\")\n",
    "        print(f\"    Statistical significance: p={impact['p_value']:.3f} [{sig}]\")\n",
    "        print(f\"    Sample sizes: pre={impact['pre_count']}, post={impact['post_count']}\")\n",
    "\n",
    "    print(f\"\\nMethodological Notes:\")\n",
    "    print(\"  - Used Welch's t-test (robust to unequal variances)\")\n",
    "    print(\"  - Statistical assumptions validated (normality, independence)\")\n",
    "    print(\"  - Effect sizes calculated for clinical interpretation\")\n",
    "    print(\"  - Bonferroni correction applied for multiple comparisons\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: processed_reddit not found. Run Reddit analysis first.\")"
   ],
   "id": "c0373c843f1dda14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ADVANCED REDDIT TOPIC MODELING\n",
    "print(\"Running Reddit Topic Modeling Analysis...\")\n",
    "\n",
    "from analyze import analyze_reddit_topics\n",
    "\n",
    "# Use the data BEFORE sentiment processing (has text columns)\n",
    "if 'fixed_df' in locals():\n",
    "    print(\"Using raw Reddit data with text columns...\")\n",
    "    topic_results = analyze_reddit_topics(fixed_df, num_topics=5, notebook_plot=True)\n",
    "\n",
    "    if topic_results:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TOPIC MODELING INSIGHTS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(\"\\nKey Findings:\")\n",
    "        for topic_id, words in topic_results['topics'].items():\n",
    "            topic_num = int(topic_id.split('_')[1])\n",
    "            if topic_results['topic_sentiment'] is not None and topic_num in topic_results['topic_sentiment'].index:\n",
    "                sentiment = topic_results['topic_sentiment'].loc[topic_num, 'mean']\n",
    "                count = topic_results['topic_sentiment'].loc[topic_num, 'count']\n",
    "                print(f\"  {topic_id}: {words[:3]} (sentiment: {sentiment:.3f}, posts: {count})\")\n",
    "            else:\n",
    "                # Get count from topic distribution instead\n",
    "                count = topic_results['topic_subreddit_dist'].loc[topic_num].sum()\n",
    "                print(f\"  {topic_id}: {words[:3]} (posts: {count}, sentiment: N/A)\")\n",
    "\n",
    "elif 'drive_data' in locals():\n",
    "    print(\"Using original Google Drive data...\")\n",
    "    topic_results = analyze_reddit_topics(drive_data, num_topics=5, notebook_plot=True)\n",
    "else:\n",
    "    print(\"ERROR: No suitable data found. Run Reddit EDA cell first.\")\n",
    "    print(\"Available variables:\", [var for var in locals() if 'reddit' in var.lower() or 'data' in var.lower()])"
   ],
   "id": "dee8afa165fb1ab0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ADVANCED REDDIT TEMPORAL PATTERN ANALYSIS\n",
    "print(\"Running Reddit Temporal Pattern Analysis...\")\n",
    "\n",
    "from analyze import analyze_temporal_patterns\n",
    "\n",
    "# Use processed_reddit which has sentiment scores\n",
    "if 'processed_reddit' in locals():\n",
    "    temporal_results = analyze_temporal_patterns(processed_reddit, notebook_plot=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEMPORAL PATTERN INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\nAnalysis Period: {temporal_results['date_range']} ({temporal_results['total_days']} days)\")\n",
    "\n",
    "    print(f\"\\nPeak Discussion Times:\")\n",
    "    peak_hour = temporal_results['hourly_activity'].idxmax()\n",
    "    peak_day = temporal_results['daily_activity'].idxmax()\n",
    "    print(f\"  Busiest hour: {peak_hour}:00 ({temporal_results['hourly_activity'].max()} posts)\")\n",
    "    print(f\"  Busiest day: {peak_day} ({temporal_results['daily_activity'].max()} posts)\")\n",
    "\n",
    "    print(f\"\\nFDA Event Impacts:\")\n",
    "    for event, stats in temporal_results['event_impacts'].items():\n",
    "        change = ((stats['post_event'] - stats['pre_event']) / stats['pre_event']) * 100\n",
    "        print(f\"  {event}: {stats['pre_event']} â†’ {stats['post_event']} posts ({change:+.1f}% change)\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: processed_reddit not found. Run Reddit EDA cell first.\")"
   ],
   "id": "bcc19cf2a8bfc0dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ADVANCED REDDIT-GOOGLE TRENDS CORRELATION\n",
    "print(\"Running Reddit-Google Trends Correlation Analysis...\")\n",
    "\n",
    "from analyze import correlate_reddit_trends\n",
    "\n",
    "# Need both processed_reddit and processed_trends\n",
    "if 'processed_reddit' in locals() and 'processed_trends' in locals():\n",
    "    correlation_results = correlate_reddit_trends(processed_reddit, processed_trends, notebook_plot=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CROSS-PLATFORM CORRELATION INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\nAnalysis Period: {correlation_results['aligned_period']}\")\n",
    "\n",
    "    print(f\"\\nCorrelation Results:\")\n",
    "    for term, corrs in correlation_results['correlation_results'].items():\n",
    "        print(f\"  {term}: Volume r={corrs['volume_correlation']:.3f}\")\n",
    "\n",
    "    if correlation_results['best_correlations']:\n",
    "        best_term, best_corr = correlation_results['best_correlations'][0]\n",
    "        print(f\"\\nStrongest Correlation: {best_term} (r={best_corr['volume_correlation']:.3f})\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Need both processed_reddit and processed_trends. Run both EDA cells first.\")"
   ],
   "id": "7a62ee5b70a50541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ADVANCED REDDIT NETWORK ANALYSIS\n",
    "print(\"Preparing data for Network Analysis...\")\n",
    "\n",
    "# Create combined_text column from existing text_to_analyze\n",
    "if 'combined_text' not in fixed_df.columns:\n",
    "    print(\"Creating combined_text column from text_to_analyze...\")\n",
    "    fixed_df['combined_text'] = fixed_df['text_to_analyze'].fillna('')\n",
    "    # Remove any posts with empty text\n",
    "    fixed_df = fixed_df[fixed_df['combined_text'].str.strip() != '']\n",
    "    print(f\"Created combined_text column. Remaining posts: {len(fixed_df)}\")\n",
    "\n",
    "print(\"Running Reddit Network Analysis...\")\n",
    "\n",
    "from analyze import analyze_subreddit_networks\n",
    "\n",
    "# Use fixed_df which now has the combined_text column\n",
    "if 'fixed_df' in locals():\n",
    "    network_results = analyze_subreddit_networks(fixed_df, notebook_plot=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NETWORK ANALYSIS INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\nNetwork Structure:\")\n",
    "    metrics = network_results['network_metrics']\n",
    "    print(f\"  Subreddits: {metrics['nodes']}\")\n",
    "    print(f\"  Connections: {metrics['edges']}\")\n",
    "    print(f\"  Network density: {metrics['density']:.3f}\")\n",
    "    print(f\"  Average connections: {metrics['average_degree']:.1f}\")\n",
    "\n",
    "    print(f\"\\nKey Hubs (Most Connected):\")\n",
    "    top_hubs = sorted(network_results['degree_centrality'].items(),\n",
    "                     key=lambda x: x[1], reverse=True)[:5]\n",
    "    for subreddit, centrality in top_hubs:\n",
    "        print(f\"  {subreddit}: {centrality:.3f}\")\n",
    "\n",
    "    print(f\"\\nInformation Bridges (High Betweenness):\")\n",
    "    top_bridges = sorted(network_results['betweenness_centrality'].items(),\n",
    "                        key=lambda x: x[1], reverse=True)[:3]\n",
    "    for subreddit, centrality in top_bridges:\n",
    "        print(f\"  {subreddit}: {centrality:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: fixed_df not found. Run Reddit EDA cell first.\")"
   ],
   "id": "a00c3ea89f4075c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create Reddit summary table\n",
    "from analyze import create_reddit_summary_table\n",
    "\n",
    "if all(var in locals() for var in ['advanced_reddit_results', 'temporal_results', 'topic_results', 'network_results', 'correlation_results']):\n",
    "    summary_table = create_reddit_summary_table(\n",
    "        advanced_reddit_results,\n",
    "        temporal_results,\n",
    "        topic_results,\n",
    "        network_results,\n",
    "        correlation_results\n",
    "    )\n",
    "    print(\"Reddit summary table generated successfully!\")\n",
    "else:\n",
    "    print(\"Missing some analysis results for summary table generation\")\n",
    "    print(\"Available variables:\", [var for var in ['advanced_reddit_results', 'temporal_results', 'topic_results', 'network_results', 'correlation_results'] if var in locals()])"
   ],
   "id": "9ee677395d582d75",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af9a8ee56dbdf81c",
   "metadata": {},
   "source": [
    "# PubMed Analysis\n",
    "pubmed_data = get_pubmed_data()\n",
    "if pubmed_data is not None:\n",
    "    processed_pubmed = process_pubmed_data(pubmed_data)\n",
    "\n",
    "    # Use ALL data for the analysis (not filtered)\n",
    "    print(f\"Analyzing {len(processed_pubmed)} total PubMed publications\")\n",
    "\n",
    "    # Add Data Preview:\n",
    "    print(f\"PubMed data shape: {processed_pubmed.shape}\")\n",
    "    print(\"First 3 rows of PubMed data:\")\n",
    "    print(processed_pubmed[['title', 'publication_date', 'journal']].head(3))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Process PubMed data using the publication rate analysis\n",
    "    from analyze import analyze_pubmed_publication_rate\n",
    "    analyze_pubmed_publication_rate(processed_pubmed, notebook_plot=True)\n",
    "    print(\"PubMed analysis completed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Advanced PubMed analysis\n",
    "pubmed_advanced_results = advanced_pubmed_analysis(processed_pubmed, notebook_plot=True)\n",
    "print(f\"Total publications analyzed: {pubmed_advanced_results['total_publications']}\")"
   ],
   "id": "5f8f93bfcd2c11b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a0b7ac8025e11b0",
   "metadata": {},
   "source": [
    "# Stock Data Analysis\n",
    "stock_data = get_stock_data()\n",
    "if stock_data is not None:\n",
    "    processed_stocks = process_stock_data(stock_data)\n",
    "\n",
    "    # Add Data Preview:\n",
    "    print(f\"Stock data shape: {processed_stocks.shape}\")\n",
    "    print(\"First 3 rows of stock data:\")\n",
    "    print(processed_stocks.head(3))\n",
    "    print(\"Stock price ranges:\")\n",
    "    print(f\"NVO: ${processed_stocks['NVO_Close'].min():.2f} - ${processed_stocks['NVO_Close'].max():.2f}\")\n",
    "    print(f\"MDGL: ${processed_stocks['MDGL_Close'].min():.2f} - ${processed_stocks['MDGL_Close'].max():.2f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    analyze_stock_and_events(processed_stocks, notebook_plot=True)\n",
    "    print(\"Stock analysis completed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Advanced Stock Analysis\n",
    "from analyze import advanced_stock_analysis\n",
    "stock_advanced_results = advanced_stock_analysis(processed_stocks, notebook_plot=True)"
   ],
   "id": "fbf0777f98993fa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Advanced Stock Volatility Analysis\n",
    "from analyze import advanced_stock_volatility_analysis\n",
    "volatility_results = advanced_stock_volatility_analysis(processed_stocks, notebook_plot=True)"
   ],
   "id": "4f80dac180602406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from analyze import cross_platform_correlation_analysis\n",
    "\n",
    "def fixed_cross_platform_correlation(processed_data: dict, notebook_plot=False):\n",
    "    \"\"\"Fixed version of cross-platform correlation analysis\"\"\"\n",
    "    print(\"\\n[Advanced Analysis] Cross-Platform Correlation Analysis...\")\n",
    "\n",
    "    save_dir = RESULTS_DIR / \"stock_analysis\"\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Extract data\n",
    "    df_stocks = processed_data.get('stocks')\n",
    "    df_trends = processed_data.get('trends')\n",
    "    df_reddit = processed_data.get('reddit')\n",
    "\n",
    "    if df_stocks is None:\n",
    "        print(\"ERROR: Stock data not available\")\n",
    "        return None\n",
    "\n",
    "    # Prepare stock returns\n",
    "    df_stocks['NVO_Returns'] = df_stocks['NVO_Close'].pct_change() * 100\n",
    "    df_stocks['MDGL_Returns'] = df_stocks['MDGL_Close'].pct_change() * 100\n",
    "    stock_returns = df_stocks[['NVO_Returns', 'MDGL_Returns']].dropna()\n",
    "\n",
    "    correlation_results = {}\n",
    "\n",
    "    # CORRELATION WITH GOOGLE TRENDS\n",
    "    if df_trends is not None:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STOCK RETURNS vs GOOGLE TRENDS CORRELATION\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        trends_daily = df_trends.resample('D').mean().ffill()\n",
    "\n",
    "        start_date = max(stock_returns.index.min(), trends_daily.index.min())\n",
    "        end_date = min(stock_returns.index.max(), trends_daily.index.max())\n",
    "\n",
    "        stock_aligned = stock_returns.loc[start_date:end_date]\n",
    "        trends_aligned = trends_daily.loc[start_date:end_date]\n",
    "\n",
    "        print(f\"Aligned period: {start_date.date()} to {end_date.date()} ({len(stock_aligned)} days)\")\n",
    "\n",
    "        trend_correlations = {}\n",
    "        for trend_col in ['MASLD', 'NAFLD', 'Rezdiffra', 'Wegovy', 'Ozempic']:\n",
    "            if trend_col in trends_aligned.columns:\n",
    "                nvo_corr = stock_aligned['NVO_Returns'].corr(trends_aligned[trend_col])\n",
    "                mdgl_corr = stock_aligned['MDGL_Returns'].corr(trends_aligned[trend_col])\n",
    "\n",
    "                trend_correlations[trend_col] = {\n",
    "                    'NVO_Correlation': nvo_corr,\n",
    "                    'MDGL_Correlation': mdgl_corr\n",
    "                }\n",
    "\n",
    "                print(f\"{trend_col}: NVO={nvo_corr:.3f}, MDGL={mdgl_corr:.3f}\")\n",
    "\n",
    "        correlation_results['google_trends'] = trend_correlations\n",
    "\n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        corr_data = []\n",
    "        for trend, corrs in trend_correlations.items():\n",
    "            corr_data.append([corrs['NVO_Correlation'], corrs['MDGL_Correlation']])\n",
    "\n",
    "        corr_df = pd.DataFrame(corr_data,\n",
    "                              index=trend_correlations.keys(),\n",
    "                              columns=['NVO Returns', 'MDGL Returns'])\n",
    "\n",
    "        sns.heatmap(corr_df, annot=True, cmap='RdBu_r', center=0,\n",
    "                   vmin=-1, vmax=1, square=True)\n",
    "        plt.title('Stock Returns vs Google Search Interest Correlation', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        trends_corr_path = save_dir / \"cross_platform_trends_correlation.png\"\n",
    "        plt.savefig(trends_corr_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Saved trends correlation to: {trends_corr_path.name}\")\n",
    "\n",
    "    # FIXED CORRELATION WITH REDDIT SENTIMENT\n",
    "    if df_reddit is not None and 'sentiment_score' in df_reddit.columns:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STOCK RETURNS vs REDDIT SENTIMENT CORRELATION\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Prepare Reddit sentiment\n",
    "        df_reddit['timestamp'] = pd.to_datetime(df_reddit['timestamp'])\n",
    "        reddit_daily = df_reddit.set_index('timestamp')['sentiment_score'].resample('D').mean()\n",
    "\n",
    "        # Align time periods with proper handling of missing dates\n",
    "        start_date = max(stock_returns.index.min(), reddit_daily.index.min())\n",
    "        end_date = min(stock_returns.index.max(), reddit_daily.index.max())\n",
    "\n",
    "        stock_aligned = stock_returns.loc[start_date:end_date]\n",
    "        reddit_aligned = reddit_daily.loc[start_date:end_date]\n",
    "\n",
    "        # FIX: Ensure both series have the same dates by reindexing\n",
    "        common_dates = stock_aligned.index.intersection(reddit_aligned.index)\n",
    "        stock_final = stock_aligned.loc[common_dates]\n",
    "        reddit_final = reddit_aligned.loc[common_dates]\n",
    "\n",
    "        print(f\"Aligned period: {start_date.date()} to {end_date.date()}\")\n",
    "        print(f\"After alignment: {len(stock_final)} matching days\")\n",
    "\n",
    "        # Calculate correlations\n",
    "        nvo_sentiment_corr = stock_final['NVO_Returns'].corr(reddit_final)\n",
    "        mdgl_sentiment_corr = stock_final['MDGL_Returns'].corr(reddit_final)\n",
    "\n",
    "        sentiment_correlations = {\n",
    "            'NVO_Sentiment_Correlation': nvo_sentiment_corr,\n",
    "            'MDGL_Sentiment_Correlation': mdgl_sentiment_corr\n",
    "        }\n",
    "\n",
    "        correlation_results['reddit_sentiment'] = sentiment_correlations\n",
    "\n",
    "        print(f\"Reddit Sentiment Correlation: NVO={nvo_sentiment_corr:.3f}, MDGL={mdgl_sentiment_corr:.3f}\")\n",
    "\n",
    "        # FIXED: Scatter plots with properly aligned data\n",
    "        if len(stock_final) > 0 and len(reddit_final) > 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "            # NVO vs Sentiment\n",
    "            ax1.scatter(reddit_final.values, stock_final['NVO_Returns'].values, alpha=0.6, s=30)\n",
    "            ax1.set_xlabel('Reddit Sentiment Score')\n",
    "            ax1.set_ylabel('NVO Daily Returns (%)')\n",
    "            ax1.set_title(f'NVO Returns vs Reddit Sentiment\\n(correlation: {nvo_sentiment_corr:.3f})')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # MDGL vs Sentiment\n",
    "            ax2.scatter(reddit_final.values, stock_final['MDGL_Returns'].values, alpha=0.6, s=30, color='orange')\n",
    "            ax2.set_xlabel('Reddit Sentiment Score')\n",
    "            ax2.set_ylabel('MDGL Daily Returns (%)')\n",
    "            ax2.set_title(f'MDGL Returns vs Reddit Sentiment\\n(correlation: {mdgl_sentiment_corr:.3f})')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            sentiment_corr_path = save_dir / \"cross_platform_sentiment_correlation.png\"\n",
    "            plt.savefig(sentiment_corr_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"Saved sentiment correlation to: {sentiment_corr_path.name}\")\n",
    "        else:\n",
    "            print(\"WARNING: No overlapping data for Reddit sentiment scatter plots\")\n",
    "\n",
    "    # CORRELATION BETWEEN STOCKS\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTER-STOCK CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    stock_correlation = stock_returns['NVO_Returns'].corr(stock_returns['MDGL_Returns'])\n",
    "    correlation_results['stock_correlation'] = stock_correlation\n",
    "\n",
    "    print(f\"NVO vs MDGL Returns Correlation: {stock_correlation:.3f}\")\n",
    "\n",
    "    # Stock correlation scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(stock_returns['NVO_Returns'], stock_returns['MDGL_Returns'],\n",
    "                alpha=0.6, s=30, color='green')\n",
    "    plt.xlabel('NVO Daily Returns (%)')\n",
    "    plt.ylabel('MDGL Daily Returns (%)')\n",
    "    plt.title(f'NVO vs MDGL Returns Correlation\\n(correlation: {stock_correlation:.3f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    stock_corr_path = save_dir / \"cross_platform_stock_correlation.png\"\n",
    "    plt.savefig(stock_corr_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved stock correlation to: {stock_corr_path.name}\")\n",
    "\n",
    "    # SUMMARY TABLE\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CROSS-PLATFORM CORRELATION - SUMMARY TABLE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    summary_data = []\n",
    "\n",
    "    # Add Google Trends correlations\n",
    "    if 'google_trends' in correlation_results:\n",
    "        for trend, corrs in correlation_results['google_trends'].items():\n",
    "            summary_data.append({\n",
    "                'Platform': 'Google Trends',\n",
    "                'Metric': trend,\n",
    "                'NVO_Correlation': f\"{corrs['NVO_Correlation']:.3f}\",\n",
    "                'MDGL_Correlation': f\"{corrs['MDGL_Correlation']:.3f}\"\n",
    "            })\n",
    "\n",
    "    # Add Reddit sentiment correlation\n",
    "    if 'reddit_sentiment' in correlation_results:\n",
    "        summary_data.append({\n",
    "            'Platform': 'Reddit',\n",
    "            'Metric': 'Sentiment Score',\n",
    "            'NVO_Correlation': f\"{correlation_results['reddit_sentiment']['NVO_Sentiment_Correlation']:.3f}\",\n",
    "            'MDGL_Correlation': f\"{correlation_results['reddit_sentiment']['MDGL_Sentiment_Correlation']:.3f}\"\n",
    "        })\n",
    "\n",
    "    # Add stock correlation\n",
    "    summary_data.append({\n",
    "        'Platform': 'Stocks',\n",
    "        'Metric': 'NVO vs MDGL',\n",
    "        'NVO_Correlation': f\"{correlation_results['stock_correlation']:.3f}\",\n",
    "        'MDGL_Correlation': f\"{correlation_results['stock_correlation']:.3f}\"\n",
    "    })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Save summary table\n",
    "    summary_path = save_dir / \"cross_platform_correlation_summary.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"Saved correlation summary to: {summary_path.name}\")\n",
    "\n",
    "    return correlation_results\n",
    "\n",
    "# Run the fixed correlation analysis\n",
    "correlation_results = fixed_cross_platform_correlation({\n",
    "    'stocks': processed_stocks,\n",
    "    'trends': processed_trends,\n",
    "    'reddit': processed_reddit\n",
    "}, notebook_plot=True)\n",
    "\n",
    "print(\"Cross-platform correlation analysis completed!\")"
   ],
   "id": "60cc99c4787f0102",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5eaf813fa7482dd",
   "metadata": {},
   "source": [
    "# Media Cloud\n",
    "MEDIA_CLOUD_ANALYSIS_SUBDIR = \"media_cloud_analysis\"\n",
    "\n",
    "media_cloud_available = get_media_cloud_data()\n",
    "if media_cloud_available:\n",
    "    try:\n",
    "        from analyze import analyze_media_cloud_timeline, analyze_media_cloud_sources\n",
    "\n",
    "        # Add Data Preview:\n",
    "        print(\"Media Cloud datasets loaded:\")\n",
    "        from analyze import load_media_cloud_datasets\n",
    "        datasets = load_media_cloud_datasets()\n",
    "        for name, data in datasets.items():\n",
    "            if 'counts' in data:\n",
    "                print(f\"  {name}: {data['counts'].shape[0]} time points\")\n",
    "            if 'sources' in data:\n",
    "                print(f\"  {name}: {data['sources'].shape[0]} unique sources\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        print(\"Running Media Cloud timeline analysis...\")\n",
    "        timeline_results = analyze_media_cloud_timeline(notebook_plot=True)\n",
    "\n",
    "        print(\"Media Cloud analysis completed!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n=== MEDIA CLOUD FDA EVENT IMPACT ANALYSIS ===\")\n",
    "media_cloud_results = advanced_media_cloud_event_analysis(notebook_plot=True)"
   ],
   "id": "95682cf3e5b40bac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n=== MEDIA CLOUD COVERAGE CONCENTRATION ANALYSIS ===\")\n",
    "concentration_results = advanced_media_cloud_concentration_analysis(notebook_plot=True)"
   ],
   "id": "58847655b924dd48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n=== MEDIA CLOUD TOPIC PROPAGATION ANALYSIS ===\")\n",
    "propagation_results = advanced_media_cloud_topic_propagation(notebook_plot=True)"
   ],
   "id": "a0cf783680d61bdc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
